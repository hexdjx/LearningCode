{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfc2ec1",
   "metadata": {},
   "source": [
    "# PyTorch 基础知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad3c01e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytracking\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ae0e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# print(torch.__version__)\n",
    "# print(torch.version.cuda)\n",
    "# print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d056d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可复现性\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "# torch.cuda.manual_seed(0)\n",
    "# 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88fb24dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.76405235]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) \n",
    "a = np.random.randn(1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1253e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c378bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1799,  0.3567, -0.9931], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISBLE_DEVICES'] = '1' # 命令行指定gpu device: CUDA_VISBLE_DEVICES=0,1 python *.py\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tensor = torch.randn(3) \n",
    "# print(tensor)\n",
    "tensor = tensor.to(device)\n",
    "print(tensor)\n",
    "# print(tensor.cuda()) # .cpu ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52900f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensor\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c57e2477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.IntTensor\n",
      "torch.Size([2, 3, 4])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 张量处理 \n",
    "tensor = torch.randn(2, 3, 4)#(1,3,224, 224) # 1, 1024, 14, 14 \n",
    "# print(tensor)\n",
    "print(tensor.type()) \n",
    "tensor = tensor.int() \n",
    "print(tensor.type()) \n",
    "\n",
    "# print(tensor.float().type()) # float()\n",
    "print(tensor.shape) #size() \n",
    "print(tensor.dim()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3dba3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAUhklEQVR4nO3ce/ju9Zzv8ddXmaKJyp6SIocKGYWllJRjcshQRIYll05qlCnKolSUrJGaUSwptSVttWWEDkTIIToNQm3bJIPoMGPZk0lrxHf/Ya73vvyz1/3+XNe297Wvx+Pv+/n7rPvw+726/+gzzfM8BwCS3Ov/9j8AgP93GAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAypoLP3J6RvuHT0+5sd3kq7f2myQ3DPwveP9l6jfHvb7fnHdMv3n5+v0mSTLwnOYtz243L/+LvdvNee++st0kyZwd2s3SR/dfiI+MfIim/r/t0/lG/5wkL8gJ7WZV+u/ThfMm7WbFdFq7OXWD17abJNnl3H5z2zs26Edf/UW/ydhzyo391+89r96r3RzyjY+u9jG+KQBQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgBlmud5sVvAphv6P/xbj2k3eVw/SZIt07/Ubbsl/cvCvnNdO8n9D+hftLbZRgM32yU55+0D0cA9cEOm5411d1zSTu5/bP+Yp7+v/0JceGz/fZqPHXvBpwee1G6+eusb281T5q3bTXa6vt+s00+S5Gmf7TdfGjhng8+f0m5+9ayPD5yUvGL7K9rNWecMHLT56j97vikAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAZeEL8baa3tb+4Tf87bHtZp1D20mS5K6lA9GHj28n8647t5uXX7ZTuzkvYxfiJXcNNPdtFyNXuk1nDERJsu/D28mG0w/bze3tImMvxMhFZkn+/FX95rsD/8Cnzf3P3pdGXohp036TJO+5pZ3Mr+8fMw08pbUO7DdJsuoNa7WbeYtV/YMW+HPvmwIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAZeFbUkcu7ZwGom037J+TJNfs1W/WOqV/DeKq9fvP6TebtJPc+3v9JknWnX/Qbu6ct2g3H57e2m42fNNx7SZJnvM3/Wb+3Fbt5ie73NBuHtIukjMOGIiS7Hva/u1mmk4fO6zpWwO3pD5u8Cbg+TOPbTfTc77TbpZtdGq7eeetu7SbJMl04UC0rJ+4JRWADqMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAWXPRB14xcHnVfu0iOf32gSjJ9J/7zaqB53Tiyv459x5o9r2sf8FYkmx9Wf85HTxwzpSBy+3W3WPgpOTg/H27mXbpX4iXA1/ab95/TDs5+EV39c9Jsvkn1xmo+p+jHfLxdvPJPLLd5IPb9Zskee/V7WTjL/Q/r8uf0f/NWD52x9+Q+ckv+D/yc31TAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAMrCF+I9beSnD9zpdsZ0wchJmX/3knZz/sA5Lxto3nj+Je1mevYf72atQ969rN1c+IXl7ea9d63XbpLk1H0GojP7n6OHXd9vbn7Mwr9C5UdXHtVukuSBVz9joNqzXRw591+H749c4PiTsc/4kov6zc8vurndzAN/wKaBSzaT5Lic2T/rynPbzSLPyDcFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoEzzPC9069NuH+1f9HTxXz6s3SzJD9tNklyX/dvNRTmj3ezWLpKhmwEHL9bK8/vJfHG/uXC+tt3sdfRD+wclufu4s9vNlDcMnDTyPo0Ye2/XG3hvV27Xb+4+rd+s/bN+s8/Y3Zc5a8/+Z++ze/af1K4f+2C7yQU795sk80u+3G72yp3t5rz5T1f7GN8UAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKEYBgLLwhXjTyB1eA/eLzRuNXRY2nTRw2NKRs05uF+tcd1i7+dWSdpIkme45od2csuY7280hh/cv43rNie0kSXLWffvv7XxX/72dhi7Ee1u7eMI2xw6ck6z7qz9pN5vd9Mx2c/N8abv5ysCv0mk39pskOeDRW7Wbja+8od3ccV07yW8PHrxU8Q0DL+BJz2on8/y51T7GNwUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAypqLPvDAgcv/9hy4WnXsjtRkXrq0f9anBw762B/nxtP3DN3YmWTNj7eTgx/av/H0kH9pJ3lXru9HSZb828Dn6KEDr99F/XPW3r1/zKpv95skechuq9rNvW/qn3NuNms3Dznux+3mvY9uJ0mSu+YftZvtBn6fVu7QTvKVg/tNkuSFP+o3Jx07eNj/nm8KABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQJnmeV7opqiTHtX/4W/8/sD1duv3kySZfzHWdU3TzgPVl/vJ4H148/TEgeradvGgJ/RP+fk/9JskGbk/bpuhkwZf9KZPbHLjULf7LecOVMf3kzX6v7cvv7r/2n30CYPXX67XTx75y37zun6Sgwef0vuX9ZsDl/df80X+2vumAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKAJSFL8SbvjFwqdsOT20n6+W1/XOSrMxp7Wbs7qqBS9OuHjhpuzf3mySZ39lPprXbzZS7281pR2zbbpLkte+6ZqAauCzs1Q9vN9OHdmg32fKefpMkDzi/33z9HwcOOqVdrD+f2m5WTmMXEM4Dv7kjv+s7Zkm7+druOw2clMyf+n67mX57af+cBf7c+6YAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAlIUvxLvjxA+2f/iG992v3eTUsUuy8th+ssMF/Wuyvp739Q+a+xegzdOm/XOSTAd9sh/dc3a/+cDAFWNjNxAOGvkcndEunv+J/dvNxZe0k98749f9ZpOB9/aWr/Sb3NZP5s8PnJOcsGO/ect3PtuP/nXXfvOlfpIkeXr/83riM57Ubt54+VWrfYxvCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgCUNRd94IZH9G88PaFdJG+ex67S/NpfXdtunjJwztb5q3az/Vv6NyDeJx9uN0mSFf1bMR+TL/TP+UA/+V4/SZL8+8CNpyc/r/85uv70dpI1xi6zHXL6X9+n3ez/d/1zHrB5v/nnT/Sb0VtzH7/0sHbz5Cv7N55eOT273WS+rN8k2WP5/drNI5bdOXTW6vimAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKAJSFL8TL+v1Lyd6ysp3kv+WMfpTkFSt+3W76z2jsDq/r37nlQPWDgSaZT35Vu5kOHbjE615/0k7enn/vn5PkGwPNsku+2I82fXo7Wat/SuahT14yPbbfPHXgE3vFI/uXKuaxe7eTKXf0z0mSc/5srGv61JnfbDdHTRcNnbVLDmw3l2zSvxBv9wUe45sCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUKZ5nhe6nWunqX+x1lfbRcZuqUuS6UntZOXcb940vafdbDlwKdkbRi9NGzhr7GLAfjVn2cBJyZSl7ebsI/+83Zz2jnaSTw68Dhv+ZuRaxeRp9/5Zu/nipb9rN9NzL2k3mffvN2MvQ3LcQHPUwLWKF2/fTn6529jv7XoDL8Y/DtwL+IjbV//v800BgGIUAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKGsu+sB18+n2D59zcrtZduQX202S/E2uajeXTP3m9JzSbrJN/5Ks737rV/1zkuSH/bNOecTD2s38mf4FXuvs2k5+b9qgney9df+Y+YZz2s201Rb9g7bpJ0nypWzcj84cuXVu034yLek3t17Xb5Jko8P7zbRJO5nn/udu5ELKJDlwoDnzjv7v+gkLPMY3BQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKBM8zwvdKvS9JuBi54u7Sfvf2G/SZLnvbbfbPagfrPt0f3m6mzYbqbc3j8oSfKZgeY5g2d17TJUrf2qy9vN3R9+d7s5bD603Zy8rJ1kvnns0rTpohf1o3+7sJ3Mz7p/u5ku/2b/nPxZu0mSaV53IBo46OEL3xf6vyy9Z+Cg5LiBv5Vvvap/Id4ihW8KABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKAJSFb0m9bepfM3jBYde3m21P3rrdJMlx2aPdrPrbv283n8vF7ebInT/ebp775LPaTZJ8alX/5sR3DVwh+eb+Mbli8DLWKz87cFg2GWge3i62ztXt5vvzF9tNktydHdvNNI3cpHl8u0k+MdBsO9AkN+UD7WbzkYPm7/ebaexD/s6XHtdulm34yv5Bp67+8+CbAgDFKABQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFAWvhBv4M60TPfpR/Pb+uckyXTEkwaqq9rFgq/WH5imP+1Hz7+z3yQ58+L+a77P5wcOevRa/WaTowYOSpbcZ/1+dNfr2sl1Axet/dcfH9BuXrrZke3m997RTw7qX+CYFf33ac6h7WaaX9VukuSYB/d/Caef9n8vjsnAObcO/KFMkgc+s538cy5vNw9Y4A+YbwoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAWfhCvBVT/6KnK/OpdrM8f9FukuTggebCnNhulp2zbrtZvnTfdpN5Sb9Jks2+3W/+qZ+sPz293fwij+oflGSa39+PVh7dbzYYuI3x+IEL0M4buFUxyaav7Tc/7d8LmJHbL781cMo2uX6gSvKk/sWA01Xn9c959MB7e+PYe7vkXbe0m8sO37TdbLDAJX++KQBQjAIAxSgAUIwCAMUoAFCMAgDFKABQjAIAxSgAUIwCAMUoAFCMAgBl4Qvxlg5ciPeRdpHMc/+yqyQ55oIj283b9xy4JCt79ZOfnN5vNt2z3ySZP7deP3r2Je1kyvPbzbz02naTJPnwE9vJ9N1ft5urt39Qu9nuzpXt5nmD/yl28Rr9z9Fp0/7t5sB7Ri516/992GjglCS5beBXcB74VR+4Dm/Yxuf2m8e+ov8+fXaBxDcFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoKy56AM/cnn/8qX5me0k07SkHw16We7Xbs4b2NE9H9y/lGyn7NdukuSUgWu8bhs45+KBO9Om/foX2yVJ9tyl31xwn3ZyYvpP6i/v1X+9b573aDdJMp3R/xzlqIE36vaBq+DW6Ce3/Xbk4r1k/lX/3/eieZuBk77dTwZv0dtzi36zSY4fOOmo1T7CNwUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAyjTP80JXFV4z9a//++1W27Wb7b93dbtJklsGmoe+qd/cc02/ufTxL2k3zznpgv5BSabpCQPVWu1iPu6H/WN+O3IfazIdO3A7aB7cTza+tp3MP7+n3UwDr/fvD7uynRyY5e1mxbRru5keuHG7yYb9JElyfT85K+9uN6/ZcYd2M39tx3aTJNN/7zfzcwYOumn1f+59UwCgGAUAilEAoBgFAIpRAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQDKwhfiZeBCvH4x7oh8qN2866y9+we9pv+sLsxj2s0L8712kyTvOfNf2s3r93lAuzlofki7OXjlP7WbJNlqg30Hqqe2i0vyqnbzn9pFsvvXB6Ik9x34hfrBTwcO6t/fmDxooPnZQJMkWexP1h8UV6/XbqYr/ke7Oefww9pNkizNHv1ovae0k3mlC/EAaDAKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAlIUvxJsGLsTLmf2Lq7bep39Mkvx4oPllVg5U67eL+/VfhvzrdEg/SpKNT+03Px/4B+bj7WKNfHfgnOSVdx3Tbs7ebODzens/yTv6yUeOGjgnyStzULt5VFa0mxsf304y/UO/Gfn7kCTzvt9qN9OLB57UGf1k4M/Df9h0oPlJu1jkFfdNAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKEYBgGIUAChrLvrAOce3f/iDz+lfSnZ9PtZukiRvemI7mZcf0W4+tGKbdvPW9G9AO3rsrrA8dOAeuJsHzjk872s39+QLAycl0337T2qe9+ufM43cgNZ/o7bIOQPnJMnSgaJ/Id70zXaSeeq/Dldk4MOa5Ka8vh/9pv/vm/d6dbuZLju73STJzpv1L7e74pcDr98vV/86+KYAQDEKABSjAEAxCgAUowBAMQoAFKMAQDEKABSjAEAxCgAUowBAMQoAFKMAQJnmeV7o+sBp4EK+eeN+NP28f85/nNZPdv52/5QvP67dTD9oJ5k3f0E/SjJtcFK7ee/KLdvN6xb72PyhDcZuxczKgWZ+cb+Z3tpO1pof1272euX57SZJNn5b//O6fPPPtZv54de0m2nkPRppkhwy0OyWD7SbZ+eAgZPGjFyKPGXX/jnzZ1b7GN8UAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKEYBgLLwhXiHp3+Z2YnTQe0mWTHQJGsPXCm1ah64oG36YL85f992Mr+sf0ySLBt4Tsun/mv3d5e3k/z1M5/Yj5JMuXao+2M4O9u2m5/l6qGz3nz2wOd176MHTrpwoOlf1nevkVvgkvzuoP7rMK+4tN1M03PbzQWfbydJkhd/p/9iTIcOvA4L/Ln3TQGAYhQAKEYBgGIUAChGAYBiFAAoRgGAYhQAKEYBgGIUAChGAYBiFAAoC1+IB8D//3xTAKAYBQCKUQCgGAUAilEAoBgFAIpRAKAYBQCKUQCg/E9eS7lRGLznDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# python image libirary PIL<-->numpy<-->torch Image:[0,1] float <--> [0,255] int\n",
    "\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tensor = torch.randn(3,28,28)\n",
    "\n",
    "tensor = tensor.cuda()\n",
    "ndarray = tensor.cpu().numpy()\n",
    "# print(ndarray)\n",
    "tensor = torch.from_numpy(ndarray) # b c h w  256 3 244 244 --> 256 10 \n",
    " \n",
    "# [H W 3]-->[3 H W] # unsqueeze(0) [1, 3, H, W]\n",
    "image = PIL.Image.fromarray(torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())\n",
    "# image1 = torchvision.transforms.functional.to_pil_image(tensor) #与上面等价\n",
    "\n",
    "# image.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# cv2.imshow('im', np.asarray(image))\n",
    "# key = cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0999611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "tensor = torch.from_numpy(np.asarray(IL.ImagPe.open('img/fig.jpg'))).permute(2,0,1).float()/255\n",
    "# print(tensor)\n",
    "tensor1 = torchvision.transforms.functional.to_tensor(PIL.Image.open('img/fig.jpg'))\n",
    "# print(tensor1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5827cfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4963])\n",
      "0.49625658988952637\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "print(torch.rand(1))\n",
    "torch.manual_seed(0)\n",
    "print(torch.rand(1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3be5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512])\n",
      "64\n",
      "torch.Size([64, 512])\n",
      "64\n",
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "# view/reshape 和 shape/size对比\n",
    "tensor = torch.rand(2,3,4)\n",
    "tensor = torch.reshape(tensor, (6,4)) \n",
    "tensor1 = tensor.view(6,4) \n",
    "tensor = torch.rand(64,512)\n",
    "print(tensor.shape)\n",
    "print(tensor.shape[0])\n",
    "\n",
    "print(tensor.size())\n",
    "print(tensor.size(0))\n",
    "\n",
    "print(tensor1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05b23c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 7.],\n",
      "        [3., 4., 8.],\n",
      "        [5., 6., 9.]])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([[1,2,7],[3,4,8], [5, 6, 9]])\n",
    "print(tensor)\n",
    "# tensor1 = torch.randperm(tensor.size(0))\n",
    "tensor1 = torch.tensor([[1,2,7],[3,4,8], [5, 6, 9]])\n",
    "print(tensor1.type())\n",
    "# print(tensor1) #permute\n",
    "# tensor = tensor[torch.randperm(tensor.size(0))] # tensor[:, torch.randperm(tensor.size(1))] \n",
    "# print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71097cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 7.]\n",
      " [3. 4. 8.]\n",
      " [5. 6. 9.]]\n",
      "[[5. 6. 9.]\n",
      " [3. 4. 8.]\n",
      " [1. 2. 7.]]\n"
     ]
    }
   ],
   "source": [
    "# 翻转\n",
    "# narray = tensor.numpy()\n",
    "# print(narray)\n",
    "# narray = narray[::-1, :]\n",
    "# print(narray)\n",
    "\n",
    "# tensor = torch.Tensor([[1,2,7],[3,4,8], [5, 6, 9]])\n",
    "\n",
    "print(tensor.flip(dims=(1,)))\n",
    "range(6,-1, -1) #step 6 5   0\n",
    "0 6  6 5 4 3 2 1\n",
    "# print(torch.arange(tensor.size(0)-1, -1, -1))\n",
    "# tensor = tensor[torch.arange(tensor.size(0)-1, -1, -1), :]\n",
    "# print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd26ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3.])\n",
      "tensor([1., 3.])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([1,2]) \n",
    "tensor1 = tensor#.clone() #import copy copy.deepcopy()\n",
    "tensor1[1]=3\n",
    "print(tensor)\n",
    "print(tensor1)\n",
    "\n",
    "# print(tensor.detach())\n",
    "# tensor.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c66fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 4])\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.rand(2,3,4)\n",
    "tensor2 = torch.rand(2,3,4)\n",
    "tensor_cat = torch.cat([tensor1, tensor2],dim=0)\n",
    "print(tensor_cat.shape)\n",
    "tensor_stack = torch.stack([tensor1, tensor2], dim=0) # 2 2 3 4 .reshape(*tensor_stack.shape[-2:])\n",
    "# print(tensor_stack.shape)  \n",
    "print(*tensor_stack.shape[-2:])\n",
    "# print(tensor_stack.reshape(-1, *tensor_stack.shape[-2:]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "926d77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 1, 28, 28])\n",
      "torch.Size([23, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(23,28,28) # 32 1 28 28\n",
    "index=torch.unsqueeze(tensor, dim=1) # b c h w\n",
    "print(index.shape)\n",
    "print(torch.squeeze(index).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7022e7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = np.array([[2,3],[4,5]])\n",
    "# n = n[::-1] #:: ... None\n",
    "n1 = n[...,None]\n",
    "print(n1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21895945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([0,1,4,0,6,0])\n",
    "print(tensor[torch.nonzero(tensor)]) # tensor[:]\n",
    "print(torch.nonzero(tensor==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cce520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法 \n",
    "import numpy as np  #元素乘 点乘  [2, 3] [4, 2] 8 6\n",
    "feat = torch.randn(1,3,5,5) \n",
    "k = torch.randn(1,3,1,1) \n",
    "k_1 = k.expand(1,3,5,5)\n",
    "# print(k_1\n",
    "print((feat * k_1 == feat * k).all()) # any()\n",
    "# print(torch.equal(feat * k_1,feat * k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a43a9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.,  4.],\n",
      "        [26., 10.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [15.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.Tensor([[1,2],[3,4]])\n",
    "tensor2 = torch.Tensor([[2,2],[5,1]]) \n",
    "print(torch.mm(tensor1, tensor2))\n",
    "# print(tensor1.matmul(tensor2))\n",
    "# print(torch.bmm(tensor1.unsqueeze(0), tensor2.unsqueeze(0)))\n",
    "\n",
    "print(tensor1*tensor2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93741813",
   "metadata": {},
   "source": [
    "# PyTorch搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8028ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力网络\n",
    "# Squeeze-and-Excitation Networks， which is a channel attention block.\n",
    "class SE_Block(nn.Module):\n",
    "    def __init__(self, ch_in, reduction=16):\n",
    "        super(SE_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, ch_in // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ch_in // reduction, ch_in, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)  # squeeze operation\n",
    "        y = self.fc(y).view(b, c, 1, 1)  # FC obtains channel attention weight\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "# CBAM: Convolutional Block Attention Module\n",
    "# channel and spatial attention\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channel, ratio=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        # channel attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, in_channel // ratio, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channel // ratio, in_channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # spatial attention\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # b, c, w, h = x.size()\n",
    "        # channel attention\n",
    "        avg_fc = self.fc(self.avg_pool(x))\n",
    "        max_fc = self.fc(self.max_pool(x))\n",
    "        c_out = self.sigmoid(avg_fc + max_fc)\n",
    "        x_c = x * c_out\n",
    "\n",
    "        # spatial attention\n",
    "        x_mean = torch.mean(x, dim=1, keepdim=True) \n",
    "        x_max, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([x_mean, x_max], dim=1)\n",
    "        x_s = self.sigmoid(self.conv(x_cat))\n",
    "        out = x_c * x_s\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "323a31fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "feat = torch.randn(1,16,28,28) # b c h w\n",
    "\n",
    "se = SE_Block(16)\n",
    "# print(se)\n",
    "# print(sum(p.numel() for p in se.parameters()))\n",
    "# print(\"Number of total parameter: %.4fM\" % (sum(p.numel() for p in se.parameters()) / 1e6))\n",
    "feat_att = se(feat)\n",
    "print(feat_att.shape)\n",
    "# cbam = CBAM(16)\n",
    "# print(cbam)\n",
    "# print(\"Number of total parameter: %.4fM\" % (sum(p.numel() for p in cbam.parameters()) / 1e6))\n",
    "# feat_att = cbam(feat)\n",
    "# print(feat_att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "771e92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional neural network (2 convolutional layers)\n",
    "import math\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.se_att = CBAM(16)\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.se_att1 = SE_Block(32)\n",
    "        \n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "        # Init weights  xaiver\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "#         out = self.se_att(out)\n",
    "        out = self.layer2(out)\n",
    "#         out = self.se_att1(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d278605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total parameter: 20844.0000\n",
      "0.34579014778137207\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "cnn = ConvNet()\n",
    "print(\"Number of total parameter: %.4f\" % (sum(p.numel() for p in cnn.parameters())))# / 1e6))\n",
    "# print(cnn)\n",
    "img = torch.randn(1,1,28,28)\n",
    "feat = cnn(img)\n",
    "toc = time.time()-tic\n",
    "print(toc)\n",
    "# print(feat.shape) # latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd827151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "# Common practise for initialization.\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',\n",
    "                                      nonlinearity='relu')\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "    elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(layer.weight, val=1.0)\n",
    "        torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "    elif isinstance(layer, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "\n",
    "# Initialization with given tensor. \n",
    "tensor = torch.Tensor([2,3])\n",
    "layer.weight = torch.nn.Parameter(tensor)\n",
    "layer.weight\n",
    "# layer.weight #迁移模型\n",
    "#   for p in self.feature_extractor.parameters():\n",
    "layer.weight.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ef475",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.load_state_dict(torch.load('*.pth.tar')) # map_location='cpu', strict=False\n",
    "\n",
    "# model_new代表新的模型\n",
    "# model_saved代表其他模型，比如用torch.load导入的已保存的模型\n",
    "model_new_dict = model_new.state_dict()\n",
    "model_common_dict = {k:v for k, v in model_saved.items() if k in model_new_dict.keys()}\n",
    "model_new_dict.update(model_common_dict)\n",
    "model_new.load_state_dict(model_new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练之前的数据处理 #numpy() RGB 0-1 0 255\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size=224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), #imagenet\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    " ])\n",
    " val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9436aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 432)\n",
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('img/fig.jpg')\n",
    "print(img.size)\n",
    "img1 = torchvision.transforms.RandomResizedCrop(size=224)(img)\n",
    "img2 = torchvision.transforms.RandomHorizontalFlip()(img)\n",
    "img3 = torchvision.transforms.Resize(256)(img)\n",
    "img4 = torchvision.transforms.CenterCrop(224)(img)\n",
    "print(img1.size)\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(img)\n",
    "# # plt.title(\"原图\")\n",
    "# plt.subplot(2,3,2)\n",
    "# plt.imshow(img1)\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(img2)\n",
    "# plt.subplot(2,3,4)\n",
    "# plt.imshow(img3)\n",
    "# plt.subplot(2,3,5)\n",
    "# plt.imshow(img4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f08e120",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_9808\\2934905210.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;31m# Train the model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mtotal_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #MyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i ,(images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7de1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # required_grad = True\n",
    "# Test the model\n",
    "model.eval()  # eval mode(batch norm uses moving mean/variance instead of mini-batch mean/variance) dropout\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item() #  0 1  1 0 \n",
    "\n",
    "    print('Test accuracy of the model on the 10000 test images: {} %'\n",
    "          .format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15304596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(torch.nn.Moudle): # mse\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = torch.mean((x - y) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度裁剪（gradient clipping）\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce learning rate when validation accuarcy plateau.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "for t in range(0, 80):\n",
    "    train(...)\n",
    "    val(...) \n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "# Cosine annealing learning rate.\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)\n",
    "# Reduce learning rate by 10 at given epochs.\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)\n",
    "for t in range(0, 80):\n",
    "    scheduler.step()    \n",
    "    train(...)\n",
    "    val(...)\n",
    "\n",
    "# Learning rate warmup by 10 epochs.\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)\n",
    "for t in range(0, 10):\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    val(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df72e331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1\n",
      "1 0.09000000000000001\n",
      "2 0.08100000000000002\n",
      "3 0.007290000000000002\n"
     ]
    }
   ],
   "source": [
    "# 从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]\n",
    "optimizer = SGD(model, 0.1)\n",
    "scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "for epoch in range(4):\n",
    "    print(epoch, scheduler2.get_last_lr()[0])\n",
    "    optimizer.step()\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4344232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练可视化\n",
    "# pip install tensorboard\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d926a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100): \n",
    "    writer.add_scalar('Loss/train', n_iter, n_iter) # np.random.random()\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04f7ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import collections\n",
    "# ResNet GAP feature.\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# print(model)\n",
    "model = torch.nn.Sequential(collections.OrderedDict(\n",
    "    list(model.named_children())[:-3]))\n",
    "# print(model)\n",
    "\n",
    "img = Image.open('img/fig.jpg')\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size=224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    " ])\n",
    "img = transform(img)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    feat = model(img.unsqueeze(0))\n",
    "    print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3917d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微调全连接层\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512, 100)  # Replace the last fc layer\n",
    "\n",
    "# 以较大学习率微调全连接层，较小学习率微调卷积层\n",
    "finetuned_parameters = list(map(id, model.fc.parameters()))\n",
    "conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)\n",
    "parameters = [{'params': conv_parameters, 'lr': 1e-3}, \n",
    "              {'params': model.fc.parameters()}]\n",
    "optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5c413",
   "metadata": {},
   "source": [
    "Minist for a example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af5d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45aba1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)  # 使用随机化种子使神经网络的初始化每次都相同\n",
    "\n",
    "# 超参数\n",
    "EPOCH = 1  # 训练整批数据的次数\n",
    "BATCH_SIZE = 20 # 10 20 30, 8 16 32 64 \n",
    "LR = 0.001  # 学习率 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d4e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "Using downloaded and verified file: ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f8df91b1ec4d2d93fef1d8351c5f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741ad28ee97d45bb8c0df726b483ae09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytracking\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# 下载mnist手写数据集\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',  # 保存或提取的位置  会放在当前文件夹中\n",
    "    train=True,  # true说明是用于训练的数据，false说明是用于测试的数据\n",
    "    transform=torchvision.transforms.ToTensor(),  # 转换PIL.Image or numpy.ndarray\n",
    "    download=True,\n",
    ")\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee1b49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data/\n",
      "    Split: Test\n",
      "torch.Size([2, 28, 28])\n",
      "torch.Size([10000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytracking\\lib\\site-packages\\torchvision\\datasets\\mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=False  # 表明是测试集\n",
    ")\n",
    "print(test_data)\n",
    "print(test_data.train_data[:2].shape)\n",
    "print(torch.unsqueeze(test_data.train_data, dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6bcef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytracking\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "# torch.unsqueeze(a) 是用来对数据维度进行扩充，这样shape就从(20,28,28)->(20,1,28,28) \n",
    "# 图像的pixel本来是0到255之间，除以255对图像进行归一化使取值范围在(0,1)\n",
    "test_x = torch.unsqueeze(test_data.train_data, dim=1).type(torch.FloatTensor)[:20] / 255 \n",
    "test_y = test_data.test_labels[:20]  # b, c h w 0-255 int  0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93496583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批训练 20个samples，1 channel，28x28 (20,1,28,28)\n",
    "# Torch中的DataLoader是用来包装数据的工具，它能帮我们有效迭代数据，这样就可以进行批训练 iter\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  #  训练时，为True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "903464b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                  padding=padding, dilation=dilation, bias=bias), # padding = (kernel_size-1)/2\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "    \n",
    "\n",
    "# 简单的CNN (两层卷积层+全连接层)继承nn.Module这个模块\n",
    "class CNN(nn.Module):  \n",
    "    def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "        super().__init__()\n",
    "        # Conv2d->BN->ReLU\n",
    "        self.conv1 = conv_bn_relu(1, 16)   # 5, 1, 2 \n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = conv_bn_relu(16, 32)\n",
    "        # 建立全卷积连接层\n",
    "        self.fc = nn.Linear(32 * 7 * 7, 10)  # 输出是10个类 \n",
    "\n",
    "        # Init weights \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x): # y = f(x)\n",
    "        x = self.pooling(self.conv1(x))  \n",
    "        x = self.pooling(self.conv2(x))\n",
    "\n",
    "        # 把每一个批次的每一个输入都拉成一个维度，即(batch_size,32*7*7)\n",
    "        # 因为pytorch里特征的形式是[bs,channel,h,w]，所以x.size(0)就是batchsize\n",
    "        x = x.reshape(x.size(0), -1) #reshape view\n",
    "        output = self.fc(x) \n",
    "        return output # 32 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3c6bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f083eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_paras_count(net):\n",
    "    \"\"\"cnn参数量统计\"\"\"\n",
    "    # Find total parameters and trainable parameters\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    print(\"Number of total parameter: %.2fM\" % (total_params / 1e6))\n",
    "    total_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    print(\"Number of trainable parameter: %.2fM\" % (total_trainable_params / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a99869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total parameter: 0.02M\n",
      "Number of trainable parameter: 0.02M\n"
     ]
    }
   ],
   "source": [
    "cnn_paras_count(cnn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e5883cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器选择Adam\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) #SGD adam adamW \n",
    "# 损失函数 \n",
    "loss_func = nn.CrossEntropyLoss()  # 目标标签是one-hotted 0, 1 , 2  0.1, 0.2, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1981397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 8, 8, 5, 8, 4, 0, 5, 8, 5, 4, 5, 5, 4, 0, 3, 8, 0, 4],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = cnn(test_x)\n",
    "# print(test_output.shape)\n",
    "# print(torch.max(test_output, 1))\n",
    "# print(torch.max(test_output, 1)[1])\n",
    "pred_y = torch.max(test_output, 1)[1].numpy()\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02cc16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | train loss: 2.6432 | test accuracy: 0.00\n",
      "Epoch:  1 | train loss: 0.2416 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.2677 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.1221 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.1004 | test accuracy: 0.90\n",
      "Epoch:  1 | train loss: 0.0472 | test accuracy: 0.90\n",
      "Epoch:  1 | train loss: 0.1938 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.1032 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.1966 | test accuracy: 0.90\n",
      "Epoch:  1 | train loss: 0.0554 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.1460 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.3187 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0110 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.1113 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.6403 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0136 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.1413 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.1351 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0055 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0032 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0064 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0049 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.2664 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0024 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0409 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0114 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0331 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0245 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0076 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0454 | test accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "#traing\n",
    "# 把x和y 都放入Variable中，然后放入cnn中计算output，最后再计算误差\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):  # 分配batch data\n",
    "        output = cnn(b_x)  # 先将数据放到cnn中计算output\n",
    "        loss = loss_func(output, b_y)  # 输出和真实标签的loss，二者位置不可颠倒\n",
    "        optimizer.zero_grad()  # 清除之前学到的梯度的参数\n",
    "        loss.backward()  # 反向传播，计算梯度\n",
    "        optimizer.step()  # 应用梯度\n",
    "            \n",
    "        if step % 100 == 0:\n",
    "            test_output = cnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == test_y.data.numpy()).sum() / test_y.size(0))\n",
    "            print('Epoch: ', epoch+1, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')#保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "754ad47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction number: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 5 4]\n",
      "real number: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# 加载模型，调用时需将前面训练及保存模型的代码注释掉，否则会再训练一遍\n",
    "cnn.load_state_dict(torch.load('cnn.pkl'))\n",
    "cnn.eval()\n",
    "\n",
    "# print 10 predictions from test data\n",
    "inputs = test_x[:20]  # 测试32个数据\n",
    "test_output = cnn(inputs)\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "print('prediction number:',pred_y)  # 打印识别后的数字\n",
    "print('real number:', test_y[:20].numpy()) # 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76e0c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 28, 28])\n",
      "torch.Size([3, 92, 242])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "img = torchvision.utils.make_grid(inputs, padding=2)\n",
    "print(img.shape)\n",
    "img = img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "cv2.imshow('win', img)  # opencv显示需要识别的数据图片\n",
    "key = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e13a99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bccc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b57baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d45d1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('waste3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29ba58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.CenterCrop((512, 512)).forward(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33d094bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.RandomCrop((512, 512)).forward(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db7d93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.RandomHorizontalFlip(p=0.999999).forward(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38c54492",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.RandomVerticalFlip(p=0.999999).forward(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66e0aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.RandomRotation(30).forward(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f24e2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.GaussianBlur(3).forward(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f870c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytracking]",
   "language": "python",
   "name": "conda-env-pytracking-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
